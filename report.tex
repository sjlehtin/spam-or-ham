\documentclass[a4paper,10pt]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[tbtags]{amsmath}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{eucal}
\usepackage{hyperref, url}
\usepackage{float}
\usepackage{verbatim} % For verbatiminput.
\usepackage{fancyvrb}

\pdfinfo{            
          /Title      (T-61.3050 Machine Learning: Basic Principles)
          /Author     ()
          /Keywords   ()
}

\DefineVerbatimEnvironment {code}{Verbatim}
   {%numbers=left,numbersep=2mm,
     %frame=lines,framerule=0.1mm, 
     fontsize=\small}
\RecustomVerbatimCommand{\VerbatimInput}{VerbatimInput}
   {%numbers=left,numbersep=2mm,
     frame=lines,framerule=0.1mm, fontsize=\small}
\DefineVerbatimEnvironment {outputlog}{Verbatim}
  {frame=lines,framerule=0.1mm, fontsize=\small}

\parindent 0mm
\parskip 3mm

%------------------------------------------------------------------------------
% First page
%------------------------------------------------------------------------------

% add your student number in parenthesis
\title{T-61.3050 Term Project, final report\\
       Using decisiontrees for spam classification} % Dictated in instructions.
\author{Jori Bomanson (81819F) \\
  {\tt jori.bomanson@aalto.fi} \\
  \\
  Sami J. Lehtinen (44814P)\\ 
  {\tt sjl@iki.fi} \\
}
\begin{document}

\floatstyle{plain}
\newfloat{Listing}{t}{lol}
\floatname{Listing}{Listing}

\maketitle
\thispagestyle{empty}
\pagebreak
\pagenumbering{arabic}

\section{Abstract}
% Include the key points of your work in a few sentences (including methods
% used and conclusions).
We trained a univariate binary classification tree to detect spam email.


\section{Why did we select Decision Trees}

The choice of using a decision tree to solve a task with boolean variables
seemed natural. Such a tree could definitely match whatever complexity there
would be in the training data.
In fact we figured our success would be determined mainly by how we would
manage to avoid overfitting.

Decisiontrees were also tempting in that they are much easier to
visualize than, e.g., naive bayes classifiers.

\section{Principles of decision trees}

\subsection{Calculating impurity}

We chose to measure impurity  with the entropy function (Quinlan 1986)
\begin{equation*}
\begin{split}
\mathcal{I}_m &= - \sum_{i=1}^K p_m^i \log_2 p_m^i  \\
\text{for a node} \quad & m  \\
\text{where} \quad 0 \log 0 &\equiv 0  \\
p_m^i &= P(\text{Instance reaching node m belongs to class i})  \\
K &= 2
\end{split}
\end{equation*}

\subsection{Pruning the tree}

\subsubsection{Prepruning}

\subsubsection{Postpruning}
\label{sect:postpruning}

\section{Validation of our Approach}

\subsection{K-fold cross validation}

We implemented 10-fold cross validation to our classifier.  This
gave more stable values for generalization error when compared to
repeatedly training the classifier without the cross validation.

One part of the training data set was used for postpruning the tree, see
section \ref{sect:postpruning}.

\subsection{Dummy model}


\section{Results with different training set sizes}

XXX results to a table.

1000 items.

\begin{outputlog}
Chosen classifier has accuracy of 1.0.
Classified as spam: 5777 / 9000
Validation:
 Classified as spam: 5777 / 9000
 Classified as spam in validation set: 6134 / 9000
 Correctly classified: 8475
 Correctly classified as spam: 5693
 Accuracy: 0.942
 Precision: 0.985
 Recall: 0.928

real	0m44.221s
user	0m44.040s
sys	0m0.150s
\end{outputlog}

4000 items in training set.

\begin{verbatim}
Chosen classifier has accuracy of 0.9725.
Classified as spam: 3984 / 6000
Validation:
 Classified as spam: 3984 / 6000
 Classified as spam in validation set: 4064 / 6000
 Correctly classified: 5798
 Correctly classified as spam: 3923
 Accuracy: 0.966
 Precision: 0.985
 Recall: 0.965

real	9m41.857s
user	9m40.900s
sys	0m0.520s
\end{verbatim}

6000 items in training set.

\begin{verbatim}
Chosen classifier has accuracy of 0.978333333333.
Classified as spam: 2677 / 4000
Validation:
 Classified as spam: 2677 / 4000
 Classified as spam in validation set: 2725 / 4000
 Correctly classified: 3886
 Correctly classified as spam: 2644
 Accuracy: 0.972
 Precision: 0.988
 Recall: 0.970

real	19m40.578s
user	19m37.980s
sys	0m1.100s
\end{verbatim}

\section{Comments on project difficulty}

\begin{thebibliography}{9}
\bibitem{alpaydin2004}
  Ethem Alpaydin,
  \emph{Introduction to Machine Learning}.
  Massachusetts Institute of Technology, Cambridge, Massachusetts,
  1st edition,
  2004. 415 pages. ISBN 0-262-01211-1.
%\bibitem{press07}
%  William H. Press, Saul A. Teukolsky, William T. Vetterling, Brian P. Flannery,
%  \emph{Numerical recipes - the art of scientific computing}.
%  Cambridge University Press, New York,
%  3rd edition,
%  2007. 1235 pages. ISBN 978-0-521-88068-8.
%\bibitem{mellin10a}
%  Ilkka Mellin, 
%  \emph{Todennäköisyyslaskenta ja tilastotiede: Kaavat}.
%  Otaniemi, 2010. 390 pages.
%\bibitem{mellin10b}
%  Ilkka Mellin, 
%  \emph{Tilastolliset taulukot}.
%  Otaniemi, 2010. 13 pages.
\end{thebibliography}

\appendix
\section{Python code for the classifier}
\verbatiminput{decisiontree.py}

\end{document}
