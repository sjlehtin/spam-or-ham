\documentclass[a4paper,10pt]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[tbtags]{amsmath}
\usepackage{amssymb}
\usepackage{hyperref, url}
\usepackage{float}

\usepackage{fancyvrb}

\pdfinfo{            
          /Title      (T-61.3050 Machine Learning: Basic Principles)
          /Author     ()
          /Keywords   ()          
}

\DefineVerbatimEnvironment {code}{Verbatim}
   {%numbers=left,numbersep=2mm,
     %frame=lines,framerule=0.1mm, 
     fontsize=\small}
\RecustomVerbatimCommand{\VerbatimInput}{VerbatimInput}
   {%numbers=left,numbersep=2mm,
     frame=lines,framerule=0.1mm, fontsize=\small}
\DefineVerbatimEnvironment {log}{Verbatim}
  {frame=lines,framerule=0.1mm, fontsize=\small}

\parindent 0mm
\parskip 3mm

%------------------------------------------------------------------------------
% First page
%------------------------------------------------------------------------------

% add your student number in parenthesis
\title{T-61.3050 Term Project, final report} % Dictated in instructions.
\author{Jori Bomanson (81819F) \\
  {\tt jori.bomanson@aalto.fi} \\
  \\
  Sami J. Lehtinen (44814P)\\ 
  {\tt sjl@iki.fi} \\
}
\begin{document}

\floatstyle{plain}
\newfloat{Listing}{t}{lol}
\floatname{Listing}{Listing}

\maketitle

\section*{Abstract}
% Include the key points of your work in a few sentences (including methods
% used and conclusions).

\section*{}

\section*{Why did we select Decision Trees}

The choice of using decision trees to solve a task with boolean variables
seemed natural. They could definitely match whatever complexity there would
be in the data. %, as opposed to the naive Bayesian classifier. tai ei.
In fact we figured our success would be determined mainly by how we would
manage to avoid overfitting, which seemed like a plausible task to do once
we would have a pure tree working.


\section*{Principles of the Method (Decision Trees)}

\section*{Validation of our Approach}

\section*{}

XXX prepruning

XXX postpruning

XXX K-fold cross validation

XXX dummy model

Lessons learned:

Check that classifying against a test set converges, i.e., doesn't
fluctuate too much, after a stable algorithm is found.

\newpage
\section*{Comments on Project Difficulty}




\begin{thebibliography}{9}
\bibitem{alpaydin2004}
  Ethem Alpaydin,
  \emph{Introduction to Machine Learning}.
  Massachusetts Institute of Technology, Cambridge, Massachusetts,
  1st edition,
  2004. 415 pages. ISBN 0-262-01211-1.
%\bibitem{press07}
%  William H. Press, Saul A. Teukolsky, William T. Vetterling, Brian P. Flannery,
%  \emph{Numerical recipes - the art of scientific computing}.
%  Cambridge University Press, New York,
%  3rd edition,
%  2007. 1235 pages. ISBN 978-0-521-88068-8.
%\bibitem{mellin10a}
%  Ilkka Mellin, 
%  \emph{Todennäköisyyslaskenta ja tilastotiede: Kaavat}.
%  Otaniemi, 2010. 390 pages.
%\bibitem{mellin10b}
%  Ilkka Mellin, 
%  \emph{Tilastolliset taulukot}.
%  Otaniemi, 2010. 13 pages.
\end{thebibliography}

\end{document}
