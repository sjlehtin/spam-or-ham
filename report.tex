\documentclass[a4paper,10pt]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[tbtags]{amsmath}
\usepackage{amssymb}
\usepackage{hyperref, url}
\usepackage{float}

\usepackage{fancyvrb}

\pdfinfo{            
          /Title      (T-61.3050 Machine Learning: Basic Principles)
          /Author     ()
          /Keywords   ()          
}

\DefineVerbatimEnvironment {code}{Verbatim}
   {%numbers=left,numbersep=2mm,
     %frame=lines,framerule=0.1mm, 
     fontsize=\small}
\RecustomVerbatimCommand{\VerbatimInput}{VerbatimInput}
   {%numbers=left,numbersep=2mm,
     frame=lines,framerule=0.1mm, fontsize=\small}
\DefineVerbatimEnvironment {log}{Verbatim}
  {frame=lines,framerule=0.1mm, fontsize=\small}

\parindent 0mm
\parskip 3mm

%------------------------------------------------------------------------------
% First page
%------------------------------------------------------------------------------

% add your student number in parenthesis
\title{T-61.3050 Term Project, final report} % Dictated in instructions.
\author{Jori Bomanson (81819F) \\
  {\tt jori.bomanson@aalto.fi} \\
  \\
  Sami J. Lehtinen (44814P)\\ 
  {\tt sjl@iki.fi} \\
}
\begin{document}

\floatstyle{plain}
\newfloat{Listing}{t}{lol}
\floatname{Listing}{Listing}

\maketitle

\section*{Abstract}
% Include the key points of your work in a few sentences (including methods
% used and conclusions).

\section*{}

\section*{Why did we select Decision Trees}

The choice of using decision trees to solve a task with boolean variables
seemed natural. They could definitely match whatever complexity there would
be in the data. %, as opposed to the naive Bayesian classifier. tai ei.
In fact we figured our success would be determined mainly by how we would
manage to avoid overfitting, which seemed like a plausible task to do once
we would have a pure tree working.


\section*{Principles of the method (decision trees)}

\subsection*{Calculating impurity}

\subsection*{Prepruning}

\subsection*{Postpruning}

\section*{Validation of our Approach}

\subsection*{K-fold cross validation}

We implemented 10-fold cross validation to our classifier.

\subsection*{Dummy model}


\section{Results with different training set sizes}

4000 items in training set.

Chosen classifier has accuracy of 0.9775.
Classified as spam: 4035 / 6000
Validation:
 Classified as spam: 4035 / 6000
 Classified as spam in validation set: 4064 / 6000
 Correctly classified: 5819
 Correctly classified as spam: 3959
 Accuracy: 0.970
 Precision: 0.981
 Recall: 0.974

\newpage
\section*{Comments on Project Difficulty}



\begin{thebibliography}{9}
\bibitem{alpaydin2004}
  Ethem Alpaydin,
  \emph{Introduction to Machine Learning}.
  Massachusetts Institute of Technology, Cambridge, Massachusetts,
  1st edition,
  2004. 415 pages. ISBN 0-262-01211-1.
%\bibitem{press07}
%  William H. Press, Saul A. Teukolsky, William T. Vetterling, Brian P. Flannery,
%  \emph{Numerical recipes - the art of scientific computing}.
%  Cambridge University Press, New York,
%  3rd edition,
%  2007. 1235 pages. ISBN 978-0-521-88068-8.
%\bibitem{mellin10a}
%  Ilkka Mellin, 
%  \emph{Todennäköisyyslaskenta ja tilastotiede: Kaavat}.
%  Otaniemi, 2010. 390 pages.
%\bibitem{mellin10b}
%  Ilkka Mellin, 
%  \emph{Tilastolliset taulukot}.
%  Otaniemi, 2010. 13 pages.
\end{thebibliography}

\end{document}
